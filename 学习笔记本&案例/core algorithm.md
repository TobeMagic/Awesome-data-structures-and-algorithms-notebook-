##  数据结构必备经法

### **学习数据结构与算法的理由**

1. 大厂面试

比如 BAT、Google、Facebook，面试的时候都喜欢考算法、让人现场写代码。公司只能考察他们的基础知识是否牢固。社招就更不用说了，越是厉害的公司，越是注重考察数据结构与算法这类基础知识。相比短期能力，他们更看中你的**长期潜力。**

2. 业务开发性能优化

对于大部分业务开发来说，我们平时可能更多的是利用已经封装好的现成的接口、类库来堆砌、**翻译业务逻辑**，很少需要自己实现数据结构和算法。但是，不需要自己实现，并不代表什么都不需要了解。

如果不知道这些类库背后的原理，不懂得时间、空间复杂度分析，如何能用好、用对它们？存储某个业务数据的时候，你如何知道应该用 ArrayList，还是 Linked List 呢？调用了某个函数之后，你又该**如何评估代码的性能和资源的消耗呢？**

作为业务开发，我们会用到各种框架、中间件和底层系统，比如 Spring、RPC 框架、消息中间件、Redis 等等。在这些基础框架中，一般都揉和了很多基础数据结构和算法的设计思想。比如，我们常用的 Key-Value 数据库 Redis 中，里面的有序集合是用什么数据结构来实现的呢？为什么要用跳表来实现呢？为什么不用二叉树呢？
如果你能弄明白这些底层原理，你就能更好地使用它们。**即便出现问题，也很容易就能定位**。因此，掌握数据结构和算法，不管对于阅读框架源码，还是理解其背后的设计思想，都是非常有用的。

而在面对新的问题或者业务场景中，也能够设计更好的方案来解决问题

> 在平时的工作中，数据结构和算法的应用到处可见。我来举一个你非常熟悉的例子：如何实时地统计业务接口的 99% 响应时间？
> 你可能最先想到，每次查询时，从小到大排序所有的响应时间，如果总共有 1200 个数据，那第 1188 个数据就是 99% 的响应时间。很显然，每次用这个方法查询的话都要排序，效率是非常低的。但是，如果你知道“堆”这个数据结构，用两个堆可以非常高效地解决这个问题。

3. 提升代码水平（个人能力！）

达到开源水平的代码能力，高手之间的竞争其实就在细节。这些细节包括：你用的算法是不是够优化，数据存取的效率是不是够高，内存是不是够节省等等。这些累积起来，决定了一个框架是不是优秀。

何为编程能力强？代码的可读性好、健壮、还是扩展性好等等，**其中性能好坏起码是其中一个非常重要的评判标准**。而对代码的时间复杂度、空间复杂度分析才能写出高性能的代码

如果你在一家成熟的公司，或者 BAT 这样的大公司，面对的是千万级甚至亿级的用户，开发的是 TB、PB 级别数据的处理系统。性能几乎是开发过程中时刻都要考虑的问题。一个简单的 ArrayList、Linked List 的选择问题，就可能会产生成千上万倍的性能差别。这个时候，数据结构和算法的意义就完全凸显出来了。之前你可能需要费很大劲儿来优化的代码，需要花很多心思来设计的架构，用了数据结构和算法之后，很容易就可以解决了。

掌握了数据结构与算法，你看待问题的深度，解决问题的角度就会完全不一样。因为这样的你，就像是站在巨人的肩膀上，拿着生存利器行走世界。数据结构与算法，会为你的编程之路，甚至人生之路打开一扇通往新世界的大门。

### 数据结构与算法心法

> **核心学习数据结构与算法**

 10 个数据结构：`数组、链表、栈、队列`（线性表）、散列表、二叉树、堆、跳表、图、Trie树

>  分析问题，⼀定要有递归的思想，⾃顶向下，从抽象到具体，这些多样化的数据结构，究其源头，都是在链表或者数组上的特殊操作

10 个算法：递归、排序、二分查找、搜索、哈希算法、贪心算法、分治算法、回溯算法、动态规划、字符串匹配算法

- **算法**是求解问题的一系列计算步骤，用来将输入数据转换成输出结果 `从蛮力到策略`
- **数据结构**是数据的组织与存储：`从杂乱无章到井然有序`
- **算法 + 数据结构 = 程序**

> 对于每一种数据结构或算法学习它的**“来历”“自身的特点”“适合解决的问题”以及“实际的应用场景”**,千万不要被动地记忆，要多辩证地思考，多问为什么。如果你一直这么坚持做，你会发现，等你学完之后，写代码的时候就会不由自主地考虑到很多性能方面的事情，时间复杂度、空间复杂度非常高的垃圾代码出现的次数就会越来越少。你的编程内功就真正得到了修炼。

<img src="core%20algorithm.assets/MergedImages.png" alt="MergedImages" style="zoom:50%;" />

> 算法的基本要素：一是 **（对数据对象的）运算和操作** ，二是 **（算法的）控制结构**

运算和操作：算术运算、逻辑运算、关系运算、数据传输

控制结构：顺序、选择、循环

> 算法的特征：**输入 、输出 、确定性、有限性** （、可行性 ）

- 输入性：必须有0个或多个输入（待处理信息）
- 输出性：应有一个或多个输出（已处理信息）
- 确定性：组成算法的每条指令是清晰的、无歧义
- 有穷性：算法必须总能在执行有限步之后终止
- （可行性：算法中所有的操作都必须足够基本，使算法的执行者或阅读者明确其含义以及如何执行。）

> 好算法的特征：

- 正确性：符合语法、编译通过；
- 健壮性：能**辨别不合法的输入**并做适当的处理，不至于异常退出（崩溃）
- 可读性：**结构化 + 准确的命名 + 注释**
- 效率性：速度尽可能快；存储空间尽可能少
- 

#### 算法本质

计算机和数学不同，计算机算法的本质是穷举，而普遍认为比较难的算法，比如回溯算法、动态规划、DFS 算法等，它们的本质也是穷举，只不过需要借助递归的形式，或者说是递归的思想，来实现穷举。

对于任何数据结构，其基本操作⽆⾮遍历 + 访问，再具体⼀点就是：增删查改。即尽可能⾼效地增删查改。

##### 遍历框架

数组遍历框架，典型的线性迭代结构：

```c++
void traverse(int[] arr) {
 for (int i = 0; i < arr.length; i++) {
 // 迭代访问 arr[i]
 }
}
```

链表遍历框架，兼具迭代和递归结构：

```c++
/* 基本的单链表节点 */
class ListNode {
 int val;
 ListNode next;
}
void traverse(ListNode head) {
 for (ListNode p = head; p != null; p = p.next) {
 // 迭代访问 p.val}
}
void traverse(ListNode head) {
 // 递归访问 head.val
 traverse(head.next);
}
```

⼆叉树遍历框架，典型的**⾮线性**递归遍历结构（重点二叉树），⼏乎所有⼆叉树的题⽬都是⼀套这个框架就出来了：

```c++
/* 基本的⼆叉树节点 */
class TreeNode {
 int val;
 TreeNode left, right;
}
void traverse(TreeNode root) {
 // 前序位置
 traverse(root.left);
 // 中序位置
 traverse(root.right);
 // 后序位置
}
```

⼆叉树框架可以扩展为 N 叉树的遍历框架：

```c++
/* 基本的 N 叉树节点 */
class TreeNode {
 int val;
 TreeNode[] children;
}
void traverse(TreeNode root) {
 for (TreeNode child : root.children)
 traverse(child);
}
```

N 叉树的遍历⼜可以扩展为图的遍历，因为图就是好⼏ N 叉棵树的结合体。你说图是可能出现环的？⽤个布尔数组 visited 做标记就⾏了，

所谓框架，就是套路。不管增删查改，这些代码都是永远⽆法脱离的结构，你可以把这个结构作为⼤纲，根据具体问题在框架上添加代码。

##### 学习心法

1、先学习像数组、链表这种基本数据结构的常⽤算法，⽐如单链表翻转，前缀和数组，⼆分搜索等。

因为这些算法属于会者不难难者不会的类型，难度不⼤，学习它们不会花费太多时间。⽽且这些⼩⽽美的算法经常让你⼤呼精妙，能够有效培养你对算法的兴趣。

2、学会基础算法之后，不要急着上来就刷回溯算法、动态规划这类笔试常考题，⽽应该先刷⼆叉树，先刷⼆叉树，先刷⼆叉树，重要的事情说三遍。

### 时间复杂度&空间复杂度分析

事后统计分析： 把代码跑一遍，通过统计、监控，就能得到算法执行的时间和占用的内存大小。（局限性大）

#### 时间复杂度分析

时间复杂度表示法，体现整体算法随着规模增大的趋势表现，也叫作渐进时间复杂度（asymptotic time complexity），简称时间复杂度。

> 1. 只关注循环最多的一段 代码
> 2. 加法原则：取量级最大的复杂度
> 3. 乘法原则：取量级结果最大的
>
> 其中算法复杂度运算规则如下：
>
> 1.  加法规则
>     O(f(n))+O(g(n))=O(max(f(n),g(n)))
>     O(f(n))+O(g(n))=O(f(n)+g(n))
>     如果g(n)=O(f(n))，则O(f(n))+O(g(n))=O( (f(n))
> 2.  乘法规则
>     O(f(n))*O(g(n))=O(f(n)*g(n))
>     O(c*f(n))=O(f(n))，c是一个正常数
> 3.  f(n)=O ( (f(n))

![Θ,O,Ω的图像表示](core%20algorithm.assets/20211016202335.jpeg)

1）渐近符号的含义：**O（上界）、Ω（下界）、Θ（准确界）** 

2）运行时间记号的定义(设有函数 f(n)和 g(n)是定义在非负整数集合上的正函数)：

> **大** **O** **记号**：存在正常数 c 和 n0 使得对所有n ≥ n0有：f(n) ≤ cg(n)，记为f(n) ∈ O(g(n))
>
> - 例如：n ∈O(n2)、100n+5 ∈O(n2)、n(n-1)/2 ∈O(n2)

> **大** **Ω** **记号**：存在正常数 c 和 n0 使得对所有n ≥ n0有：f(n) ≥ cg(n)，记为f(n) ∈ Ω(g(n))
>
> - 例如：n3∈Ω (n2)、n(n+1)∈Ω (n2)、4n2+5 ∈Ω (n2)

> **大Θ记号**：设有函数 f(n)和 g(n)是定义在非负整数集合上的正函数，如果存在正整数 n0和正常数c1 和 c2（c1 ≤c2），使得当 n≥n0 时，有 c1 g(n)≤f(n)≤c2 g(n) ，就称 f(n)的阶是 Θ(g(n))，则记做f(n)=Θ(g(n))

<img src="core%20algorithm.assets/image-20231225103903739.png" alt="image-20231225103903739" style="zoom: 67%;" />

> 时间复杂度具有「最差」、「平均」、「最佳」三种情况，分别使用 O , Θ , Ω 三种符号表示。此外还有均摊时间复杂度（amortized timecomplexity）。「`平均则是引入概率论，均摊则是平摊思想`」

**根据从小到大排列，常见的算法时间复杂度主要有：**

**`O(1) ＜ O(log n) ＜ O(n) ＜ O(nlog n) ＜ O(n^2) ＜ O(n^3) < O(2^n) ＜ O(n!) ＜ O(n^n)`**

主要分为两类，**多项式量级**和**非多项式量级**。

<img src="core%20algorithm.assets/image-20231128154326858.png" alt="image-20231128154326858" style="zoom: 33%;" />

最常见的多项式时间算法的渐近时间复杂度。
        `O(1)＜O(log n)＜O(n)＜O(nlog n)＜O(n2)＜O(n3)`

最常见的非多项式量级算法的渐近时间复杂度。当数据规模 n 越来越大时，非多项式量级算法的执行时间会急剧增加，求解问题的执行时间会无限增长。所以，非多项式时间复杂度的算法其实是**非常低效**的算法。
        ` O(2n)＜O(n!)＜O(nn)`

![img](core%20algorithm.assets/20211016203924.png)

>  **渐进增长率比较**

方法1：**定义法**

找到正常数 c 和 n0 使得对所有n ≥ n0 有 f(n) ≤ cg(n)，则f(n) = O(g(n))

方法2：**极限法**

比较两个函数f(n)和g(n)的渐近增长率时，可以对两个函数相除，然后令变量 n 趋向于无穷，看这个极限值是无穷大还是一个大于零的常数还是趋向于0。

![img](https://img.jwt1399.top/img/image-20211016205209274.png)

- 前两种情况意味着f(n) ∈ O(g(n))
- 后两种情况意味着f(n) ∈ Ω(g(n))
- 第二种情况意味着f(n) ∈ Θ(g(n))

方法3：**取对数法**

对于比较难的比较的两个函数，我们可以对它们同时取对数后再进行比较

参考文章：

https://jwt1399.top/posts/46989.html#toc-heading-9

####空间复杂度分析

空间复杂度分析：时间复杂度的全称是渐进时间复杂度，表示算法的执行时间与数据规模之间的增长关系。类比一下，空间复杂度全称就是渐进空间复杂度（asymptotic spacecomplexity），表示算法的存储空间与数据规模之间的增长关系。

查看循环最多的变量生成的一段代码

<img src="core%20algorithm.assets/image-20231128154541062.png" alt="image-20231128154541062" style="zoom:50%;" />

常见的复杂度并不多，从低阶到高阶有：O(1)、O(logn)、O(n)、O(nlogn)、O(n )。几乎都是这些

> 其中还有四种复杂度分析方法
>
> 最好情况时间复杂度（best case timecomplexity）、最坏情况时间复杂度（worst case time complexity）、
>
> 

1. 数组和字符串：
  
   - **数组操作**（插入、删除、查找等）
   
   >  前缀和算法 ：
   >
   >  1.  寻找数组的中心索引 （左边之和等于右边）（前缀和）
   >
   >  排序：
   >
   >  1.  搜索插入位置（查找相同值或插值） （二分查找）
   >  1.  [合并区间](https://leetcode.cn/problems/merge-intervals/) （排序 + 二维数组）
   >
   >  
   >
   >  十大排序
   
   - 字符串操作（反转、查找、匹配等）
   - 双指针技巧（快慢指针、滑动窗口等）
   
2. 链表：
   - 单链表和双链表的基本操作
   - 快慢指针技巧在链表中的应用
   - 链表反转、环检测等问题

3. 栈和队列：
   - 栈和队列的基本操作（入栈、出栈、入队、出队等）
   - 用栈解决的问题（括号匹配、逆波兰表达式等）
   - 用队列解决的问题（广度优先搜索等）

4. 哈希表：
   - 哈希表的原理和实现
   - 哈希函数的设计和冲突解决方法
   - 哈希表在查找和去重等问题中的应用

5. 树与图：
   - 二叉树的遍历（前序、中序、后序）
   - 二叉搜索树的性质和操作
   - 堆和优先队列的基本概念和应用
   - 图的表示方法和遍历算法（深度优先搜索、广度优先搜索）

6. 排序和搜索：
   - 常见排序算法（冒泡排序、插入排序、快速排序等）
   - 高级排序算法（归并排序、堆排序等）
   - 二分查找和其他搜索算法的实现和应用

7. 动态规划：
   - 动态规划的基本概念和解题思路
   - 斐波那契数列和背包问题等经典动态规划案例
   - 动态规划的优化技巧和常见变种

8. 贪心算法：
   - 贪心算法的基本思想和适用条件
   - 贪心算法的经典案例（任务调度、区间覆盖等）
   - 贪心算法与动态规划的比较和区别

9. 图算法：
   - 最短路径算法（Dijkstra算法、Bellman-Ford算法等）
   - 最小生成树算法（Prim算法、Kruskal算法等）
   - 拓扑排序和关键路径等问题

10. 高级数据结构：
    - 并查集的原理和应用
    - 字典树和前缀树的基本操作和应用
    - 线段树和树状数组的实现和应用

## 数据结构

可以初步分为线性表或者非线性表

<img src="core%20algorithm.assets/image-20231128190250563.png" alt="image-20231128190250563" style="zoom:50%;" />

与它相对立的概念是非线性表，比如二叉树、堆、图等。之所以叫非线性，是因为，在非线性表中，数据之间并不是简单的前后关系，遍历主要使用递归方法。

<img src="core%20algorithm.assets/image-20231128190420528.png" alt="image-20231128190420528" style="zoom:50%;" />

数据结构的存储⽅式只有两种：数组（顺序存储）和链表（链式存储）

> Redis 数据库的朋友可能也知道，Redis 提供列表、字符串、集合等等⼏种常⽤数据结构，但是对于每
>
> 种数据结构，底层的存储⽅式都⾄少有两种，以便于根据存储数据的实际情况使⽤合适的存储⽅式。

### 数组

数组（Array）是一种线性表数据结构。它用一组连续的内存空间，来存储一组具有相同类型的数据。由于数组有**连续的内存空间和相同类型的数据**,内存访问机制 - 任意访问(随机访问)

> 有这么一种说法,之所以数组下标从0开始, 是因为在内存访问机制中可以减少一次减号运算
>
> 从数组存储的内存模型上来看，“**下标”最确切的定义应该是“偏移（offset）**”。前面也讲到，如果用 a 来表示数组的首地址，a[0] 就是偏移为 0 的位置，也就是首地址，a[k] 就表示偏移 k 个 type_size 的位置，所以计算 a[k] 的内存地址只需要用这个公式：
>
> `a[k]_address = base_address + k * type_size`

与之对应的也有两个问题,插入数据和删除数据,需要移动大量的内存,而实际中的动态数组需要划出大量的内存块迁移,会导致内存碎片问题, 

> 在面对这个场景, JVM 标记清除垃圾回收算法的核心思想先记录下已经删除的数据。每次的删除操作并不是真正地搬移数据，只是记录数据已经被删除。当数组没有更多空间存储数据时，我们再触发执行一次真正的删除操作，这样就大大减少了删除操作导致的数据搬移。

<img src="core%20algorithm.assets/image-20231128190517761.png" alt="image-20231128190517761" style="zoom:50%;" />

此外还要警惕数据越界问题,很多计算机病毒也正是利用到了代码中的数组越界可以访问非法地址的漏洞，来攻击系统，所以写代码的时候一定要警惕数组越界。

> 数组越界在 C 语言中是一种未决行为，并没有规定数组访问越界时编译器应该如何处理。因为，访问数组的本质就是访问一段连续内存，只要数组通过偏移计算得到的内存地址是可用的，那么程序就可能不会报任何错误。(也就是继续运行,你发现不了!!)

实际上,有很多容器已经被开发优化好,比如 Java 中的 ArrayList、C++ STL 中的vector。在项目开发中，ArrayList 最大的优势就是可以将很多数组操作的细节封装起来。比如前面提到的数组插入、删除数据时需要搬移其他数据等。另外，它还有一个优势，就是支持动态扩容(将空间自动扩容为 1.5 倍大小。)。不过，这里需要注意一点，因为扩容操作涉及内存申请和数据搬移，是比较耗时的。所以，如果事先能确定需要存储的数据大小，最好在创建 ArrayList 的时候事先指定数据大小。

1.Java ArrayList 无法存储基本类型，比如 int、long，需要封装为 Integer、Long 类，而Autoboxing、Unboxing 则有一定的性能消耗，所以如果特别关注性能，或者希望使用基本类型，就可以选用数组。

2.如果数据大小事先已知，并且对数据的操作非常简单，用不到 ArrayList 提供的大部分方法，也可以直接使用数组。

对于业务开发，直接使用容器就足够了，省时省力。毕竟损耗一丢丢性能，完全不会影响到系统整体的性能。但如果你是做一些非常底层的开发，比如开发网络框架，性能的优化需要做到极致，这个时候数组就会优于容器，成为首选。

优缺点：

> 数组由于是紧凑连续存储,可以随机访问，通过索引快速找到对应元素，⽽且相对节约存储空间。但正因为连续存储，内存空间必须⼀次性分配够，所以说数组如果要扩容，需要重新分配⼀块更⼤的空间，再把数据全部复制过去，时间复杂度 O(N)；⽽且你如果想在数组中间进⾏插⼊和删除，每次必须搬移后⾯的所有数据以保持连续，时间复杂度 O(N)。

### 链表

三种最常见的链表结构，它们分别是：单链表、双向链表和循环链表, 双向循环链表。由于链表性质, 一般不会出现内存碎片问题.

我们把内存块称为链表的“结点”。为了将所有的结点串起来，每个链表的结点除了存储数据之外，还需要记录链上的下一个结点的地址。如图所示，我们把这个记录下个结点地址的指针叫作后继指针next。

数组简单易用，在实现上使用的是连续的内存空间，可以借助 CPU 的缓存机制，**预读**数组中的数据，所以访问效率更高。而链表在内存中并不是连续存储，所以对 CPU 缓存不友好，没办法有效预读。

<img src="core%20algorithm.assets/image-20231128191817122.png" alt="image-20231128191817122" style="zoom:50%;" />

数组的缺点是大小固定，一经声明就要占用整块连续内存空间。如果声明的数组过大，系统可能没有足够的连续内存空间分配给它，导致“内存不足（out of memory）”。如果声明的数组过小，则可能出现不够用的情况。这时只能再申请一个更大的内存空间，把原数组拷贝进去，非常费时。链表本身没有大小的限制，天然地支持动态扩容，我觉得这也是它与数组最大的区别。(当我们往支持动态扩容的数组中插入一个数据时，如果数组中没有空闲空间了，就会申请一个更大的空间，将数据拷贝过去，而数据拷贝的操作是非常耗时的。)

> 如果你的代码对内存的使用非常苛刻，那数组就更适合你。因为链表中的每个结点都需要消耗额外的存储空间去存储一份指向下一个结点的指针，所以内存消耗会翻倍。而且，对链表进行频繁的插入、删除操作，还会导致频繁的内存申请和释放，容易造成内存碎片，如果是 Java 语言，就有可能会导致频繁的 GC（Garbage Collection，垃圾回收）。

代码要写好链表有以下几点：

1. 了解理解指针或引用的含义（地址调用）
2. 警惕指针丢失和内存泄漏：特别是在删除操作中，避免丢失和未释放资源
3. 利用哨兵机制简化链表代码

优缺点：

> 链表因为元素不连续，⽽是靠指针指向下⼀个元素的位置，所以不存在数组的扩容问题；如果知道某⼀元素的前驱和后驱，操作指针即可删除该元素或者插⼊新元素，时间复杂度 O(1)。但是正因为存储空间不连续，你⽆法根据⼀个索引算出对应元素的地址，所以不能随机访问；⽽且由于每个元素必须存储指向前后元素位置的指针，会消耗相对更多的储存空间。

## 算法

二叉树系列算法分为两种解题思路：

**一种是分解问题的思维模式**，这种思路代表着动态规划、分治算法等；**另一种是遍历问题的解题思维模式**，这种思路代表着回溯算法、DFS 算法等。

反过来，在用动态规划/回溯算法等比较复杂的问题时，我也会教大家用树的视角来理解算法，只要把递归树画出来，就可以很直观地理解这些递归算法：

**把递归函数理解成递归树上的一个指针，回溯算法是在遍历一棵多叉树，并收集叶子节点的值；动态规划是在分解问题，用子问题的答案来推导原问题的答案**。

![image-20231225105424184](core%20algorithm.assets/image-20231225105424184.png)

常见的问题一般都为判断问题和优化问题

- **判断问题**：是否存在一个…解
- **优化问题**：找出**最大/最小**的…解

> 很多经典的难问题都是优化问题，而一个优化问题往往可以转换成对应的判断问题。

贪心算法：逐步建立一个解决方案，具体地优化一些局部准则。 自顶向下。

分治算法：将一个问题分解成独立的子问题，求解每个子问题，并将子问题的解组合起来形成原问题的解。 自顶向下。

动态规划：把一个问题分解成一系列相互重叠的子问题，并为越来越大的子问题建立解决方案。自底向上。

### 递归算法&分治算法

> 递归与分治的定义

**递归**：一个算法直接地或间接地调用自己本身，简称自己调用自己

【所能解决问题的特征】

可分解为一个或多个**相同**子问题，递归有限性（一定的递归次数），有界性（有结束递归的条件）

**分治**：将难以直接解决的大问题，分割成一些规模较小的相同问题，以便各个击破，分而治之。

【所能解决问题的特征】

（1）该问题的**规模缩小到一定的程度**就可以容易地解决；

（2）该问题可以分解为若干个规模较小的相同问题，即该问题具有**最优子结构性质**；

（3）利用该问题分解出的**子问题的解可以合并**为该问题的解；

（4）该问题所分解出的各个**子问题是相互独立**的，即子问题之间不包含公共的子问题。

> 递归与分治的算法思想

**递归法思想**：

通过函数调用自身将问题转化为本质相同但规模较小的子问题，是分治策略的具体体现。

**分治法思想**：

将一个规模为 n 的问题分解为 k 个规模较小的子问题，这些子问题互相独立且与原问题相同。

递归地解这些子问题，然后将各子问题的解合并得到原问题的解，自底向上逐步求出原来问题。

> 1）分治法的基本步骤：
>
> （1）划分：将原问题分解为若干个规模较小，相互独立，与原问题形式相同的子问题；
>
> （2）求解子问题：若子问题规模较小而容易被解决则直接解，否则递归地解各个子问题；
>
> （3）合并：将各个子问题的解合并为原问题的解。
>
> 2）分治法的启发式规则（原则）
>
> （1）**平衡子问题**：最好使子问题的规模大致相同。
>
> （2）**独立子问题**：各子问题之间相互独立

#### 递归的复杂性分析

> 方法1：**主定理-递推方程求解**

主定理适用于求右边递归式算法的时间复杂度：**T(n) = aT(n/b) + f(n)**

其中：

- n：问题的规模大小
- a：原问题的子问题个数
- n/b：每个子问题的大小
- f(n)：将原问题分解成子问题和将子问题的解合并成原问题的解的时间

![image-20231226140437826](core%20algorithm.assets/image-20231226140437826.png)

方法2：**递归树方法（Recursion Tree）**

> **一切递归算法的本质都是一棵多叉树，你一定要用多叉树的视角去理解递归算法，这样才能形成框架思维**

在递归树中每一个节点表示一个单一子问题的代价，子问题对应某次递归函数的调用。我们将树中每层中的代价求和，得到每层的代价，然后将所有层的代价求和，得到所有递归调用的总代价。

以mergeSort为例：

$T(N)=2T(N/2) + \Theta(N)$
假设$\Theta(N)=aN$

<img src="core%20algorithm.assets/image-20231226140816742.png" alt="image-20231226140816742" style="zoom:50%;" />

树的高度为$logN$
每层代价为$aN$
可得总代价=$aN(logN)$
根据Big-O定理，可得$T(N)=O(Nlog(N))$

例子：

<img src="core%20algorithm.assets/image-20231226145913563.png" alt="image-20231226145913563" style="zoom: 33%;" />

#### 经典算法

经典算法如下：

<img src="core%20algorithm.assets/image-20231226143849657.png" alt="image-20231226143849657" style="zoom:67%;" />

<img src="core%20algorithm.assets/image-20231226144007170.png" alt="image-20231226144007170" style="zoom:67%;" />

![image-20231226144029881](core%20algorithm.assets/image-20231226144029881.png)

还有快速算法、大数乘法（分治求解，减少乘法次数）等等，以下为经典算法的时间复杂度。

![image-20231226144354146](core%20algorithm.assets/image-20231226144354146.png)

> 重点补充
>
> 1）快速排序算法的性能
>
> 快速排序算法的效率与 轴值的选择 和 划分子序列的平衡性 有关
>
> 快速排序算法的性能取决于 **划分的对称性**
>
> 2）各问题求解时所用方法或算法
>
> 递归：排列问题，整数划分问题，Hanoi 塔问题
>
> 分治策略：二分搜索技术，大整数乘法，矩阵乘法，棋盘覆盖，合并排序，快速排序，线性时间选
>
> 择，最接近点对问题，循环赛日程表。
>
> 【例题 1】快速排序算法是基于 `分治策略 `的一种排序算法。
>
> 【例题 2】从分治法的一般设计模式可以看出，用它设计出的程序一般是` 递归算法` 。

### 动态规划

动态规划问题分析是自顶而下的思路，但是算法实现却是自底而上的策略。

动态规划与分治法类似，都是把大问题拆分成小问题，通过寻找大问题与小问题的递推关系，解决一个个小问题，最终达到解决原问题的效果。

但不同的是，分治法在子问题和子子问题等上被重复计算了很多次，而动态规划则具有记忆性，通过填写表把所有已经解决的子问题答案纪录下来，在新问题里需要用到的子问题可以直接提取，避免了重复计算，从而节约了时间，所以在问题满足最优性原理之后，用动态规划解决问题的核心就在于填表，表填写完毕，最优解也就找到。

> **本章重点：**
>
> - 理解动态规划算法的思想。
> - 对相应问题能建立基本的递归关系式并用从底至上的方法来求解，在求解过程中知道如何建立数据储存的表格。
> - 重点掌握的问题：带权重的活动安排问题、**0-1背包问题**、**最长公共子序列问题**、**矩阵连乘的最优计算次序问题**。
> - 理解0-1背包问题的动态规划算法不是多项式时间算法。

> 动态规划算法的基本步骤

（1）找出最优解的性质，并刻画其结构特征。

（2）递归地定义最优值。

（3）以自底向上的方式计算出最优值。

（4）根据计算最优值时得到的信息，构造最优解。

> 动态规划的算法思想

将待求解问题分解成若干个子问题，先求解子问题，然后从这些子问题的解得到原问题的解，

【适用条件】**分解得到的子问题往往不是相互独立的**

> 动态规划算法的基本要素

动态规划算法的两个基本要素是. **最优子结构性质** 和 **重叠子问题性质**

> 动态规划法的变形： **备忘录方法**
>
> 【区别】动态规划算法是 **自底向上** ，备忘录方法（和递归）算法的是 **自顶向下** 。
>
> 动态规划每个子问题都要解一次，但不会求解重复子问题；备忘录方法只解哪些确实需要解的子问题；递归方法每个子问题都要解一次，包括重复子问题。
>
> > 备忘录方法的控制结构与直接递归方法的控制结构相同，区别在于备忘录方法为每个解过的子问题建立了备忘录以备需要时查看，避免了相同子问题的重复求解。

#### 经典算法

##### 矩阵连乘问题

> 矩阵连乘的计算次数与计算顺序的关系

假设有一个p*q规模的矩阵A，一个q*r规模的矩阵B，并且我们现在再加上一个规模为r*s的矩阵C，那么这三个矩阵的乘积ABC有两种计算顺序：

 （AB）C
   A（BC）

对于第一种计算顺序来说，总共需要的计算次数为：

因为AB共需要计算【p*q*r】次，并且生成一个规模为【p*r】的中间矩阵，这个中间矩阵再与矩阵C相乘，又需要计算【p*r*s】次，并且生成一个规模为【p*s】的矩阵，这个矩阵也就是最后的结果，因此，按照第一种顺序计算，一共需要的计算次数如下：

$mult[(AB)C] = pqr+prs$

同理，按照第二种计算顺序，我们一共需要计算下面这么多次：

$mult[A(BC)] = qrs+pqs$

假设p=5,q=4,r=6并且s=2，则：

![img](core%20algorithm.assets/20190730111806740.png)

按照第一种计算顺序，我们需要计算180次，而按照第二种计算顺序，我们只需要计算88次就够了！！但是这两种顺序计算出来的矩阵的值却始终是一样的，真是太神奇了，计算顺序竟然能够影响所需要的计算次数，**因此，对于矩阵连乘来说，计算顺序是非常重要的！**

> **问题描述：**给定 n 个矩阵 {A1,A2,…,An} ，其中 Ai 与 Ai+1 是可乘的，用加括号的方法表示矩阵连乘的次序，不同加括号的方法所对应的计算次序是不同的，求矩阵连乘的最佳计算次序。
>
>
> 问题需要我们找出最少的计算次数，就是需要我们找到“正确”的计算顺序！

1. **分析最优子结构**，在本问题中，即找出如何划分“括号”的方法。

将矩阵连乘的积 **Ai Ai+1 … Aj** 简记为`A[i][j]` ，**Ai** 的维度记为 **pi-1 × pi**，那么上述问题变为求解 `A[1][n]`的最佳计算次序。

`A[1][n]的最佳计算次序：`设这个计算次序在矩阵 AK (1≤k<n) 和 AK+1 之间将矩阵链断开，则相应的加括号方式：( **Ai Ai+1 … Ak** )( **Ak+1Ai+1 … An** )，依此计算顺序，总计算量为 **Ai Ai+1 … Ak** 的计算量加上 **Ak+1Ai+1 … An** 的计算量，再加上 ( **Ai Ai+1 … Ak** ) 和 ( **Ak+1Ai+1 … An** )相乘的计算量。即 **`A[1][n] = A[1][k] + A[k+1][n] + pi-1 pk pn`**

> 此时问题的“最优子结构”就已经出现了，为了保证Ai...k*Ak+1...j是最优的计算顺序，则Ai...k和AK+1...j也应该是由“最优计算顺序”计算出来的，因此，我们就可以递归的调用这个过程。
> 假设Ai...k的计算顺序并不是最优的，那么我们可以用更好的计算顺序去替换，这样就产生了悖论。
> 同样的，如果Ak+1...j并不是最优的，那么我们可以在找出另外一个更好的计算顺序来替换他，此时也产生了悖论。

2.**建立最优子结构的递推式**

设计算 `A[i][j]` (i≤j) 所需的最少乘法次数为 `m[i][j]`，那么得到以下的递推公式（最少乘积次数则为【Ai...k所需要的最少乘积次数】+【Ai+1...j所需要的最少乘积次数】+【中间生成的两个临时矩阵所需要的乘积次数】）：

![img](core%20algorithm.assets/20190730222401675.png)

3. **自底向上计算最优质**

K的可能值也只有j-i种，因此我们就把这j-i都一个个的去试一遍，然后找出m[i,j]最小的那一个情况，此时的K值就是我们需要的，并且最小乘积次数也找到了，就是m[i,j]次。

计算m[i,j]时，我们必须首先得把![img](core%20algorithm.assets/20190731001913900.png)和![img](core%20algorithm.assets/20190731001924521.png)

给计算出来，才能够顺利的把![img](core%20algorithm.assets/2019073100200185.png)给计算出来。

对于被分割的两个子序列，对应的矩阵链的长度都小于![img](core%20algorithm.assets/2019073100222159.png)

因此，我们的算法以矩阵链长度增长的顺序来计算，就像下面这样子（假设我们需要计算m[1,n]）：

![img](core%20algorithm.assets/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzE5NzgyMDE5,size_16,color_FFFFFF,t_70.png)

例如，要计算矩阵连乘积A1A2A3A4A5A6，其中各矩阵的维数分别为:

| A1    | A2    | A3   | A4   | A5    | A6    |
| :---- | :---- | :--- | :--- | :---- | :---- |
| 30x35 | 35x15 | 15x5 | 5x10 | 10x20 | 20x25 |

依据递推公式，按照图 a 的次序，计算出 `m[i][j]`

![img](core%20algorithm.assets/image-20211011130742451.png)

![img](https://img.jwt1399.top//img/20211011131609.png)

本题是求解 `A[1][6]` 的最佳计算次序，即求`m[1][6]` 。由图 b 可知， `m[1][6] =15125`，因此矩阵连乘积A1A2A3A4A5A6 的最优值为 **15125**

4. **构造`最优解`**

若将对应`m[i][j]`的断开位置`k`记为`s[i][j]`，计算出最优值`m[i][j]`后，可递归地由`s[i][j]`[][]构造出相应的最优解。



![img](core%20algorithm.assets/20211011131648.png)



**例如**，**`m[2][5] = m[2][3] + m[4][5] + p1p3p5`** ，则 k = 3，因此 **`s[2][5] = 3`**

★最优解：

`s[1][6] = 3` ,因此矩阵链在A3和A4之间断开，则加括号方式为 **(A1A2A3)(A4A5A6 )**

`s[1][3] = 1`,因此矩阵链在A1和A2之间断开，则加括号方式为 **(A1(A2A3))(A4A5A6 )**

`s[4][6] = 5,`因此矩阵链在A5和A6之间断开，则加括号方式为 **(A1(A2A3))((A4A5)A6 )**

因此最优解为 **(A1(A2A3))((A4A5)A6 )**

```c++
#include <iostream>
using namespace std;
#define N 100
int s[N][N], m[N][N];  //s存储切割位置，m存储最优值 

void MatricChain(int *p , int n) {//p矩阵维数数组，n为矩阵个数
    for (int i = 1; i <= n; i++) {//初始化，对角线上的计算量和加括号的位置为0
        m[i][i] = 0;
        s[i][i] = 0;
    }
    for (int r = 2; r <= n; r++) {//r为矩阵链的长度
        for (int i = 1; i <= n - r + 1; i++) { //i为首矩阵的序号
            int j = i + r - 1; //j为尾矩阵的序号
            m[i][j] = m[i][i] + m[i + 1][j] + p[i - 1] * p[i] * p[j];//首先尝试在矩阵 i 处分开
            s[i][j] = i;
            for (int k = i + 1; k < j; k++) {  
                    int t = m[i][k] + m[k + 1][j] + p[i - 1] * p[k] * p[j]; // 然后尝试在矩阵 k 处分开 (i<=k<j)
                    if (t < m[i][j]) {
                        m[i][j] = t;
                        s[i][j] = k;
                    }
            }
        }
    }
}
void Traceback(int i, int j) {
    if (i == j) {
        return;
    }
    int k = s[i][j];
    Traceback(i, k); 
    Traceback(k + 1, j);
    cout << "A" << "[" << i << ":" << k << "]" << "×" << "A""[" << (k + 1) << ":" << j << "]"<<"  " ;
}

int main(){
    int p[] = {30,35,15,5,10,20,25};//矩阵维数
    int n = sizeof(p) / sizeof(int) - 1;//矩阵个数

    for(int i = 0; i < n; i++){
        cout << p[i]<<"×"<<p[i+1]<<"  ";
    }
    cout << "这" << n << "个矩阵连乘的最优值和最优解？" << endl;
    cout << endl;

    MatricChain(p,n);
    cout << "m[i][j]:" << endl;
    for (int i = 1; i <= n; i++) {
        for (int j = 1; j <= n; j++)
            cout << m[i][j] << "\t";
        cout << endl;
    }

    cout << endl;
    cout << "s[i][j]:" << endl;
    for (int i = 1; i <= n; i++) {
        for (int j = 1; j <= n; j++)
            cout << s[i][j] << "\t";
        cout << endl;
    }
    cout << endl;
    cout << "最少连乘次数(最优值):" << m[1][n] << "次。" << endl;
    cout << "最佳计算次序(最优解):" ;
    Traceback(1, n);
    cout << endl;
}
```

![img](core%20algorithm.assets/20211011231627.png)

##### 最长公共子序列

**问题描述︰**给定两个字符串，求解这两个字符串的最长公共子序列(LCS)。如: X={1,5,2,8,9,3,6},Y={5,6,8,9,3,7}，其最长公共子序列为{5,8,9,3}，最长公共子序列长度为4。那么如何求解呢？

[1143. 最长公共子序列 - 力扣（Leetcode）](https://leetcode.cn/problems/longest-common-subsequence/description/?envType=study-plan-v2&id=top-100-liked)

**1、分析最优子结构**

设序列 **X={x1, x2, …, xi}** 和 **Y={y1, y2, …, yj}** 的最长公共子序列为 **Z={z1, z2, …, zk}**，则

- ①若 **xi=yj** ，则 zk=xi=yj 且 **Zk-1** 是 Xi-1 和 Yj-1 的最长公共子序列;
- ②若 **xi≠yj** 且 **zk≠xi** ，则 **Zk** 是 Xi-1 和 Yj 的最长公共子序列;.
- ③若 **xi≠yj** 且 **zk≠yj** ，则 **Zk **是 Xi 和 Yj-1 的最长公共子序列。

**2、建立递推公式**

用**`c[i][j]`**表示 **Xi={x1, x2, …, xi}** 和 **Yj={y1, y2, …, yj}** 的最长公共子序列的长度，那么得到以下的递推公式：



![img](core%20algorithm.assets/20211010182558.png)

**3、计算`最优值`**

> 假设 X={A,B,C,E} 和 Y={B,D,C,E}

根据上方递推公式得到下表：

| `c[i][j]` | 1B   | 2D   | 3C   | 4E   |
| :-------- | :--- | :--- | :--- | :--- |
| **1A**    | 0    | 0    | 0    | 0    |
| **2B**    | 1    | 1    | 1    | 1    |
| **3C**    | 1    | 1    | 2    | 2    |
| **4E**    | 1    | 1    | 2    | `3`  |

在 `C[2][0]` 处，j = 0 ，此时根据公式`C[2][0]= 0`

在 `C[2][1]` 处，B = B，即 **xi=yj** ，此时根据公式`C[2][1]=C[1][0]+1=0+1=1`

在 `C[2][2]` 处，B ≠ C，即 **xi≠yj** ，此时根据公式`C[2][2]=max{C[2][1],C[1][2]}=1`

根据最**右下角的值(`c[][]`)**，我们可以知道**最长公共子序列长度为3**。

**4、构造`最优解`**

`b[i][j]`记录`c[i][j]`的值是由哪个子问题的解得到的

- `if(X[i]==Y[j])` 用b=1代表
- `if(X[i]!=Y[j])` 用b=2代表

| `b[i][j]` | 1B   | 2D   | 3C   | 4E   |
| :-------- | :--- | :--- | :--- | :--- |
| **1A**    | 2    | 2    | 2    | 2    |
| **2B**    | 1    | 2    | 2    | 2    |
| **3C**    | 2    | 2    | 1    | 2    |
| **4E**    | 2    | 2    | 2    | 1    |

| `c[i][j]` | 1B   | 2D   | 3C   | 4E   |
| :-------- | :--- | :--- | :--- | :--- |
| **1A**    | 0    | 0    | 0    | 0    |
| **2B**    | 1`↖` | 1`←` | 1    | 1    |
| **3C**    | 1    | 1    | 2`↖` | 2    |
| **4E**    | 1    | 1    | 2    | 3`↖` |

`↖`处则为最长公共子序列{B,C,E}

再来几个例题：

输入：X=<A,B,C,B,D,A,B>, Y=<B,D,C,A,B,A>，

| `c[i][j]` | **0** | **1B** | **2D** | **3C** | **4A** | **5B** | **6A** |
| --------- | ----- | ------ | ------ | ------ | ------ | ------ | ------ |
| **0**     | 0     | 0      | 0      | 0      | 0      | 0      | 0      |
| **1A**    | 0     | 0      | 0      | 0      | 1      | 1      | 1      |
| **2B**    | 0     | 1      | 1      | 1      | 1      | 2      | 2      |
| **3C**    | 0     | 1      | 1      | 2      | 2      | 2      | 2      |
| **4B**    | 0     | 1      | 1      | 2      | 2      | 3      | 3      |
| **5D**    | 0     | 1      | 2      | 2      | 2      | 3      | 3      |
| **6A**    | 0     | 1      | 2      | 2      | 3      | 3      | 4      |
| **7B**    | 0     | 1      | 2      | 2      | 3      | 4      | 4      |

解：长度为4

`b[i][j]`记录`c[i][j]`的值是由哪个子问题的解得到的

- `if(X[i]==Y[j])` 用↖代表

| `b[i][j]` | 1B               | 2D             | 3C             | 4A           | 5B             | 6A               |
| --------- | ---------------- | -------------- | -------------- | ------------ | -------------- | ---------------- |
| 1A        | **B** [1,1]      | B  [1,2]       | B  [1,3]       | B  [1,4]   ↖ | B  [1,5]       | B  [1,6]   ↖     |
| [2B][2B]  | ` B  [2,1]   ↖ ` | B  [2,2]       | B  [2,3]       | B  [2,4]     | B  [2,5]   ↖   | B  [2,6]         |
| [3C][]    | B  [3,1]         | B  [3,2]       | `B  [3,3]   ↖` | B  [3,4]  ←  | B  [3,5]       | B  [3,6]         |
| [4B][]    | B  [4,1]    ↖    | B  [4,2]       | B  [4,3]       | B  [4,4]     | `B  [4,5]   ↖` | B  [4,6]         |
| 5D        | B  [5,1]         | B  [5,2  ]   ↖ | B  [5,3]       | B  [5,4]     | B  [5,5]    ↑  | B  [5,6]         |
| [6A][]    | B  [6,1]         | B  [6,2]       | B  [6,3]       | B  [6,4]   ↖ | B  [6,5]       | `B  [6,6]   ↖  ` |
| 7B        | B  [7,1  ]   ↖   | B  [7,2]       | B  [7,3]       | B  [7,4]     | B  [7,5]    ↖  | B  [7,6]    ↑    |

即 B, C, B, A 

##### 0-1背包问题

> **问题描述：**有n个物品，它们有各自的重量 wi 和价值 vi ，现有给定容量为 C 的背包，如何让背包里装入的物品具有最大的价值总和？
>
> > 在选择装入背包中的物品时，对每种物品 i 只有两种选择，即装入或者不装入背包。不能将物品 i装入背包多次，也不能只装入物品 i 的部分。所以也叫 0-1背包问题，即一个特殊的整数规划问题。

![img](core%20algorithm.assets/1.png)

**1、分析最优子结构**

定义一个参数：`OPT(i,w)`

`OPT(i,w)`表示表示前 i 个物品 ( 1,2,3,…,i )的最大价值，i（当前背包存放物品的数量）、w（当前背包容量）

OPT(i,w) 显然有两种方案：

- ①不选择 i 物品
  - 如果不选择 i 物品，原问题退化成 **OPT(i-1，w)**，即包的剩余容量比 i 物品重量小，装不下，此时的价值与前 i-1 个的价值是一样的，从(1,2,3…i-1)中找最优解
- ②选择 i 物品
  - 如果选择 i 物品，原问题退化成 **vi + OPT(i-1，w-wi)**，即既然选择了 i 物品，能装的重量减少 wi，并尝试 i-1 是否装入

**2、建立递推公式**

递推公式如下：



![img](core%20algorithm.assets/202111111714615.png)



**3、计算`最优值`、构造`最优解`**

例如：给定如下 5 个物品的价值 vi 和重量 wi，限制包的容量 C 为11



![img](core%20algorithm.assets/202111111714596.png)



依据递推公式，计算出`OPT(i,w)`

![img](core%20algorithm.assets/202111111714543.png)

OPT(2,2) = max{ v2 + OPT(1,2-w2)，OPT(1,2)} = max{6，1} = 6

…

OPT(3,5) = max{ v3 + OPT(2,5-w3)，OPT(2,5)} = max{18，7} = 18

OPT(4,11) = max{ v4 + OPT(3,11-w4)，OPT(3,11)} = max{40，25} = 40

OPT(5,11) = max{ v5 + OPT(4,11-w5)，OPT(4,11)} = max{35，40} = 40

**因此最大价值为 40，由图中红线回溯可知，背包装了物品 3 和 4**

又例如：



<img src="core%20algorithm.assets/image-20231227110303444.png" alt="image-20231227110303444" style="zoom:67%;" />

则最大价值为 6，

m[1,6]>m[2,6]故第一件物品选中，将 c 更新为 c-w1=6-2=4

m[2,4]=m[3,4]故第二件物品未选中，c 值保留不变为 4

m[3,4]>0 故第三件物品被选中

故装入背包的物品有 1 和 3

- 加餐

 



> 问题的 **最优子结构性质** 是该问题可用动态规划算法或贪心算法求解的关键特征。
>
> 动态规划和分治法在分解子问题方面的不同点是 **前者分解出的子问题有重叠的，而后者分解出的**
>
> **子问题是相互独立（不重叠）的**
>
> 动态规划和分治法的相同点是 **两者都是将待求解问题分解成若干个子问题，先求解子问题，然后**
>
> **从这些子问题的解得到原问题的解** 

参考文章：

https://blog.csdn.net/qq_19782019/article/details/94356886

https://labuladong.github.io/algo/di-er-zhan-a01c6/bei-bao-le-34bd4/jing-dian--28f3c/

https://jwt1399.top/posts/46989.html#toc-heading-57

### 贪心算法

顾名思义，贪心算法总是作出在当前看来最好的选择。也就是说贪心算法并不从整体最优考虑，它所作出的选择只是在某种意义上的局部最优选择。虽然贪心算法不能对所有问题都得到整体最优解，但对许多问题它能产生整体最优解。如单源最短路径问题，最小生成树问题等。在一些情况下，即使贪心算法不能得到整体最优解，其最终结果却是最优解的很好近似。贪心算法就是用计算机模拟一个「贪心的人」来做出决策。这个贪心的人是目光短浅的，他每次总是：

- 只做出**当前看来最好的选择**
- **只看眼前的利益，而不考虑做出选择后对未来造成的影响**
- 并且他一旦做出了选择，就**没有办法反悔**（不可回溯）

`总结：`在对问题求解时，总是做出在**当前最好的选择**。也就是说并**不从整体最优考虑**，他所做出的是在某种意义上的**局部最优解**。 因此贪心算法不是对**所有问题**都能得到整体最优解。

> 贪心算法的基本要素是 贪心选择性质 和 最优子结构性质 
>
> **贪心选择性质**：指所求问题的**整体最优解可以通过一系列局部最优的选择，即贪心选择来达到**。
>
> **最优子结构性质**：当一个问题的**最优解包含其子问题的最优**解时，称此问题具有最优子结构性质。

贪心算法则通常以自顶向下的方式进行，以迭代的方式作出相继的贪心选择，每作一次贪心选择就将所求问题简化为规模更小的子问题。 

**应用场景**

解决一个问题需要多个步骤，每一个步骤有多种选择。**想清楚局部最优，想清楚全局最优，感觉局部最优是可以推出全局最优，并想不出反例，那么就试一试贪心**。

**解题步骤**

贪心算法一般分为如下三步：

- **1.分解：**将问题分解为若干个子问题
- **2.解决：**找出适合的贪心策略，求解每一个子问题的最优解
- **3.合并：**将局部最优解堆叠成全局最优解

#### 正确性证明

对于一个具体问题，要确定它是否具有贪心选择性质，必须证明每一步所作的贪心选择最终导致问题的整体最优解。**贪心算法最难的部分不在于问题的求解，而在于正确性的证明**，一般考虑某个子问题的最优解，然后考虑用一个贪心选择替换其中某个选择，修改此解，导出更小子问最优子结构同动态规划，而且其实一般也不是特别需要证明，关键还是在证明问题具有**贪心选择性质**。

> 如果在算法领域，遇到了需要使用数学证明的问题，通常首先想到的是使用两种方式，这两种方式分别是数学归纳法和反证法。
>
> 数学归纳法相当于是递推的过程，就像动态规划的过程，将基本的问题解决之后，假设规模为n的问题可以解决，就能推导出规模为n+1这样的问题。对于数学归纳法所适用的领域，通常都是有一个变量n在逐渐增加的过程。
>
> 反证法假设它不正确，然后看可不可能推导出矛盾。

![image-20231227112752826](core%20algorithm.assets/image-20231227112752826.png)

1. 假设贪心算法得到的解不是最优解，假设 S1 是贪心算法得到的解，S2 是所有最优解中和 S1 具有最多相同元素的解；
2. 比较 S1 和 S2，观察 S1 和 S2 中第一个（最前面一个）不一样的元素；
3. 在解 S2 中将不一样的元素换成 S1 中的那个元素得到另一个最优解 S3，这样 S3 和 S1 比 S2 和 S1有更多相同元素，和假设 S2 是与 S1 有最多相同元素的最优解矛盾，这样来推导 S1 是最优解。

贪心算法最优性证明例题
首先我们引入一个问题Interval Scheduling（调度问题）

> 操作系统进程调度算法中，短作业优先可以使**平均等待时间**最小，（平均等待时间是n个进程等待CPU时间的总和除以n）。证明该算法具有贪心选择性质

现在我们有一组请求 request = {1, 2, 3, … , n}，requesti的开始时间是starti，结束时间是endi，我们的目标是从集合request中选出尽可能多的不重合的request。

```
贪心算法通常都是很自然的思路，我们可以想到

总是选择最先开始的request，这样可以使我们的资源更早的投入使用。
总是选择持续时间最短的request，这样可以使我们的资源更快的被释放，这样可以使更多的请求被接受。
总是选择冲突最少的request，这样可以使得我们拒绝更少的request。
总是选择最先结束的request。
对于每一个贪心策略，我们都需要证明它是否能够一步一步的到达最优解。
```

> 1. 我们可以首先**声明一个最优解O**。
> 2. 如果在**第一步**，可以证明贪心策略不比最优解差。
> 3. 我们假设**第k步**，贪心策略的选择也不必最优解差。
> 4. 那么我们只要证明第k+1步，贪心策略的选择也可以接受，那么就说明贪心策略可以达到最优解。
>    可以给出最优性证明。

我们期望证明它得到的可以接受的request数是A，最优解得到的可以接受的request数是O，如果我们可以证明A === O，那么就说明这个贪心策略将会得到最优解。

接下来，我们假设A中的第i个request的结束时间是endi，O中第i个request(请求)的结束时间是Endi，如果我们可以证明endi不迟于Endi，那么贪心策略会是最优的。

1. 当i === 1，因为贪心策略选择的是所有request中最早结束的request，所以end1必然不迟于End1。

2. 我们假设当i === k时，endk也不迟于Endk。

3. 我们现在要证明当i === k + 1时，endk+1不迟于Endk+1

   因为贪心策略始终选择剩下的（没有被拒绝，也没有被接受的）request中的最早完成的，又因为前一步中endk不迟于Endk，所以对于第k+1步，贪心策略也必然选择不迟于最优解的request，这样我们就证明贪心策略的最优性。

​	

> 其中在动规之间二者的区别是
>
> 对于0-1背包问题，贪心选择之所以不能得到最优解是因为在这种情况下，它无法保证最终能将背包装满，部分闲置的背包空间使每公斤背包空间的价值降低了。事实上，在考虑0-1背包问题时，应**比较选择该物品和不选择该物品所导致的最终方案，然后再作出最好选择。由此就导出许多互相重叠的子问题**。这正是该问题可用动态规划算法求解的另一重要特征。

#### 经典算法

最小生成树的Prim算法（`从初始点开始找到的路径最短`）Prim算法利用 贪心  策略求解  最小生成树  问题，其时间复杂度是 O(n2)    

最小生成树的Kruskal算法（`全局路径最短`）

>  当图的边数为e时，Kruskal算法所需的计算时间是 O(eloge) 。当 e = Ω(n^2)    时，Kruskal算法比Prim算法差，但当e = O(n^2)时，Kruskal算法却比Prim算法好得多。

单源最短路径的Dijkstra算法。

##### Dijkstra算法

荷兰杰出计算机科学家、软件工程师 [Dr. Edsger W. Dijkstra](https://en.wikipedia.org/wiki/Edsger_W._Dijkstra) 创建并发布了这个算法。

1959 年，他发表了一篇 3 页的文章《A note on two problems in connexion with graphs》来介绍他的新算法。

在 2001 年的一次采访中，Dijkstra 博士透露了他设计这个算法的起因和过程：

> 从 Rotterdam 到 Groningen 的最短路线是什么？我花了大概 20 分钟时间设计了这个寻找最短路径的算法。一天早上我正和我年轻的未婚妻在 Amsterdam 逛街，觉得有点累了，我们就坐在咖啡厅的露台上喝了一杯咖啡，我在想是否能够解决这个问题，然后，我设计出了这个最短路径算法。我说过，这是一个 20 分钟的设计。事实上，三年之后的 1959 年它才被发布，现在看来依然很不错，其原因之一是我当时设计的时候没有纸和笔，从而不得不极力避免所有可避免的复杂性。最终，令我惊讶的是，这个算法成为了我成名的基石之一。——引自文章[《An interview with Edsger W. Dijkstra》](https://dl.acm.org/doi/pdf/10.1145/1787234.1787249)

- Dijkstra 算法能够寻找出图中指定节点（“源节点”）到所有其他节点的最短路径。
- Dijkstra 算法利用边的权重来做计算，寻找源节点到所有其他节点的总距离最短（总权重最小）的路径。

##### Huffman编码

霍夫曼编码是一种无前缀编码。解码时不会混淆。其主要应用在数据压缩，加密解密等场合。

假如我有A,B,C,D,E五个字符，出现的频率（即权值）分别为5,4,3,2,1,那么我们第一步先取两个**最小权值**作为左右子树构造一个新树，即取1，2构成新树，其结点为1+2=3，如图：

![image](core%20algorithm.assets/0z710sycxq.png)

虚线为新生成的结点，第二步再把**新生成的权值为3的结点放到剩下的集合**中，所以集合变成{5,4,3,3}，再根据第二步，**取最小的两个权值构成新树**，如图：

![image](core%20algorithm.assets/yt5xtlfvo4.png)

再依次建立哈夫曼树，如下图：

![image](core%20algorithm.assets/7exutdvazl.jpeg)

其中各个权值替换对应的字符即为下图：

![image](core%20algorithm.assets/4bkr6a4l7p.jpeg)

所以各字符对应的编码为：A->11,B->10,C->00,D->011,E->010

再来一个例子

<img src="core%20algorithm.assets/image-20231227152039789.png" alt="image-20231227152039789" style="zoom:67%;" />



### 回溯法 & 分支限定法

对于许多问题，当需要找出它的解的集合或者要求回答什么解是满足某些约束条件的最佳解时，往往要使用回溯法。
这种方法适用于解一些组合数相当大的问题，具有“通用解题法”之称。

回溯和分支限界法是比较常用的对候选解进行系统检查两种方法。按照这两种方法对候选解进行系统检查通常会使问题的求解时间大大减少（无论对于最坏情形还是对于一般情形） 。可以避免对**很大的候选解集合进**行检查，同时能够保证算法运行结束时可以找到所需要的解。通常能够用来求解规模很大的问题。

#### 回溯法

其实回溯算法和我们常说的 DFS 算法非常类似，本质上就是一种暴力穷举算法。回溯算法和 DFS 算法的细微差别是：**回溯算法是在遍历「树枝」**，DFS 算法是在遍历「节点」。回溯法是一种**选优搜索法**，按选优条件向前搜索，以达到目标。但当探索到某一步时，发现原先选择并不优或达不到目标，就退回一步重新选择，这种走不通就退回再走的技术为**回溯法**，而满足回溯条件的某个状态的点称为“回溯点”。（有点类似启发式搜索，不过回退为回溯）

**把递归函数理解成递归树上的一个指针，回溯算法是在遍历一棵多叉树，并收集叶子节点的值；动态规划是在分解问题，用子问题的答案来推导原问题的答案**。

> 1. 回溯法是一种既带有 系统性 又带有 跳跃性 的搜索算法。
> 2. 以深度优先方式**系统搜索问题**解的算法称为 回溯法 。
> 3. 用回溯法解题的一个显著特征是在搜索过程中**动态产生问题的解空间**。在任何时刻，**算法只保存从根结点到当前扩展结点的路径**。

回溯法实际上一个类似枚举的搜索尝试过程，主要是在搜索尝试过程中寻找问题的解，当发现已不满足求解条件时，就“回溯”返回，尝试别的路径。

1、回溯法的基本思想

在问题的**解空间树**中，按`深度优先策略，从根结点出发搜索解空间树`。

【或】在问题的状态空间树上做带剪枝的深度优先搜索。

2、剪枝函数

回溯法搜索解空间树时，通常采用两种策略避免无效搜索，提高回溯法的搜索效率。常用的两种剪枝函数为 约束函数 和 限界函数 

一是用**约束函数**在扩展结点处剪去**不满足约束**的子树。

二是用**限界函数**剪去**得不到最优解**的子树

> N 皇后问题和 0/1 背包问题正好是两种不同的类型，其中同时使用约束条件和目标函数的界进行裁剪的是 0/1 背包问题 ，只使用约束条件进行裁剪的是 N 皇后问题 。

3、回溯法的基本步骤

（1）针对所给问题，定义问题的解空间。

> > 生成问题状态的基本方法
>
> 1. 扩展结点（E-结点, Expansion Node）
>
>    一个**正在产生儿子的结点**称为扩展结点
>
> 2. 活结点（L-结点, Live Node）
>
>    一个**自身已生成但其儿子还没有全部生成**的节点称做活结点
>
> 3. 死结点（D-结点, Dead Node）
>
>    一个**所有儿子已经产生**的结点称做死结点
>
> > 深度优先的问题状态生成法
>
> 1. 如果对一个扩展结点R，一旦产生了它的一个儿子C，就把C当做新的扩展结点。
> 2. 在完成对子树C（以C为根的子树）的穷尽搜索之后，将R重新变成扩展结点，继续生成R的下一个儿子（如果存在）。
>
> > 宽度优先的问题状态生成法
>
> 1. 一个扩展结点变成死结点之前，它一直是扩展结点。

（2）确定易于搜索的解空间树。

（3）以深度优先方式搜索解空间，并在搜索过程中用剪枝函数避免无效搜索。

**空间复杂性 **& **时间复杂性**

> 用回溯法解题的一个显著特征是在搜索过程中动态产生问题的解空间。在任何时刻，算法只保存从根结点到当前扩展结点的路径。
> 如果解空间树中从根结点到叶结点的最长路径的长度为**h(n)**，则回溯法所需的计算空间通常为**O(h(n))**。
> 显式地存储整个解空间则需要O(2^h(n))或O(h(n)!)内存空间。
>
> ----
>
> 用回溯法设计解装载问题的O(2n)计算时间算法。
> 在某些情况下该算法优于动态规划算法。

 **子集树 & 排列树**

 ***子集树***：当所给的问题是**从n个元素的集合S中找出满足某种性质的子集**时，相应的解空间称为***子集树***。例如，那个物品的0-1背包问题所相应的解空间树就是一颗子集树。这类子集问题通常有2^n^ 个叶节点，其节点总个数为2^(n+1)-1^。遍历子集树的任何算法均需要O(2^n^)的计算时间。

![img](core%20algorithm.assets/051655557113181.jpg) 

***排列树***：当所给问题是确定**n个元素满足某种性质的排列**时，相应的解空间树称为***排列树***。排列树通常有n!个叶子节点。因此遍历排列树需要O(n!)的计算时间。
   ![img](core%20algorithm.assets/051658026801559.jpg)



##### 经典算法

画解空间树，0-1 背包的解空间树（满二叉树—子集树），货郎问题，程序作业问题（排列树）

###### 0-1 **背包的解空间树**

n=3，直接画一个满 2 叉树，除根节点 A，往下每一层表示物品 1—3，左 1 右 0，1 选 0 不选。下图在 L 节点取得最优值，则最优解为：011，即选择物品 2 和 3。(**子集树**)

<img src="core%20algorithm.assets/image-20231227191525809.png" alt="image-20231227191525809" style="zoom: 67%;" />

###### **旅行商问题 货郎担问题**

有n个城市，用1，2，…，n表示，城i,j之间的距离为dij，有一个货郎从城1出发到其他城市一次且仅一次，最后回到城市1，怎样选择行走路线使总路程最短？（注：该问题还可以用动态规划法求解）

> 每个城市走一遍，最后回到住地的路线，使总的路程最短。该问题是一个 NP 完全问题， 有(n-1)!条可选路线最优
>
> **排列树**
>
> 可以看到最后的结果中，解(1,3,2,4,1)有最优值 25 

<img src="core%20algorithm.assets/image-20231227192201625.png" alt="image-20231227192201625" style="zoom:67%;" />

###### **批处理作业调度**

批处理作业调度问题要从 n 个作业的所有排列中找出有最小完成时间和的作业调度，所以批处理作业调度问题的解空间是一颗排列树。

<img src="core%20algorithm.assets/image-20231227193459864.png" alt="image-20231227193459864" style="zoom: 67%;" />

##### N皇后问题

> 问题定义：在n×n的国际象棋棋盘上摆下n个皇后，使所有的皇后都不能攻击到对方，找出所有符合要求的情况。
>
> <img src="core%20algorithm.assets/queens.jpg" alt="img" style="zoom:50%;" />

分析：n后问题的解空间树是一棵排列树，**解与解之间不存在优劣的分别**（所以采用回溯而不是分支限定）。直到**搜索到叶结点时才能确定出一组解**。

采用回溯法可以系统地搜索问题的全部解。

![image-20231228083911777](core%20algorithm.assets/image-20231228083911777.png)

#### 分支限定法

回溯法以深度优先的方式搜索解空间树，而分支限界法则以**广度优先或以最小耗费优先的方式**搜索解空间树。分支限界法中如果某孩子结点的目标函数可能取得的值超出目标函数的界，则将其丢弃，因为从这个结点生成的解不会比目前已经得到的解更好；否则，将其加入待处理结点表。该方法通过限界函数剪枝操作很好的减少了搜索数量次数，

下表列出了回溯法和分支限界法的一些区别：

| **方法**   | **对解空间树的搜索方式**   | **存储结点的常用数据结构** | **结点存储特性**                             | **常用应用**                                 |
| ---------- | -------------------------- | -------------------------- | -------------------------------------------- | -------------------------------------------- |
| 回溯法     | 深度优先搜索               | 栈                         | 活结点的所有可行子结点被遍历后才被从栈中弹出 | 找出满足约束条件的所有解                     |
| 分支限界法 | 广度优先或最小消耗优先搜索 | 队列、优先队列             | 每个结点只有一次成为扩展结点的机会           | 找出满足约束条件的一个解或特定意义下的最优解 |

> 在分支限界法中，每一个活结点只有一次机会成为扩展结点。
> 活结点一旦成为扩展结点，就一次性产生其所有儿子结点。**在这些儿子结点中，导致不可行解或导致非最优解的儿子结点被舍弃，其余儿子结点被加入活结点表中。**
> 从活结点表中取下一结点成为当前扩展结点，并重复上述结点扩展过程
> 这个过程一直持续到找到所求的解或活结点表为空时为止。

从活结点表中选择下一扩展结点的不同方式导致不同的分支限界法，常见的分支限界法分为:

1. 队列式分支限界法: 将活结点表组织成一个**队列**，并按照队列先进先出(FIFO) 原则选取下一个结点为扩展结点
2. 优先队列式分支限界法: 将活结点表组织成一个**优先队列 (用堆实现)**并选取**优先级最高**的活结点成为当前扩展结点。

##### 经典算法

有一批共n个集装箱要装上2艘载重量分别为C1和C2的轮船，其中集
装箱i的重量为Wi，且<img src="core%20algorithm.assets/image-20231228084135364.png" alt="image-20231228084135364" style="zoom: 33%;" />

装载问题要求确定是否有一个合理的装载方案可将这个集装箱装上这2艘轮船。如果有，找出一种装载方案。 
容易证明：如果一个给定装载问题有解，则采用下面的策略可得到最优装载方案。 
(1)首先将第一艘轮船尽可能装满；
(2)将剩余的集装箱装上第二艘轮船。 

不难看出，右边的左边为1即为装，只要叶子节点时剩余大于第二艘船即可



<img src="core%20algorithm.assets/image-20231229181750293.png" alt="image-20231229181750293" style="zoom:67%;" />

<img src="core%20algorithm.assets/image-20231228084554293.png" alt="image-20231228084554293" style="zoom:67%;" />

参考文章：

https://cloud.tencent.com/developer/article/1582245

https://blog.csdn.net/weixin_41387874/article/details/120052490

https://www.freecodecamp.org/chinese/news/dijkstras-shortest-path-algorithm-visual-introduction/

### Np问题

确定性问题版本的背包问题是NP的，

> 理解0-1背包问题的动态规划算法不是多项式时间算法。

P 是否等于 NP 是计算复杂度理论里面最著名的未解决的问题之一，一个 NP 完全问题，如果能找到解决它的多项式时间算法，那么就说明了 P = NP。

如今 0-1 背包问题已经被证明是 NP 完全问题，而它却有着一个动态规划解法，该解法有着 O(n*W) 的时间复杂度，其中 n 是物品的个数，W 是背包限制的最大负重。所以时间复杂度对输入 n，W 来说是多项式时间的，所以说明了 NP = P 是不是哪里出错了呢？

其实多项式时间是相对于输入规模来说的，输入规模最直观的理解就是输入到该算法的数据占了多少比特内存。0-1 背包的输入有 n 个物品的价值，n 个物品的重量，还有背包的最大负重 W。如今假设 W 占用的比特数为 L（也就是说背包的最大负重的输入规模是 L），那么 log(W) = L，所以 O(n*W) = O(n*2L)，由此看到，该算法的时间复杂度对于输入规模 L 来说是指数级别的，随着输入规模 L 的增加，运算时间会迅速增长。

实际上，人们把这种动态规划的算法称为伪多项式时间算法（pseudo-polynomial time algorithm），这种算法不能真正意义上实现多项式时间内解决问题。
